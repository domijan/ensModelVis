---
title: "Regression Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{RegressionEg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
   eval = requireNamespace("stacks", quietly = TRUE) &&
    requireNamespace("kernlab", quietly = TRUE) &&
    requireNamespace("tidymodels", quietly = TRUE),
  fig.width=8, fig.height=6
)
```

```{r setup}
library(ensModelVis)
```


An example of fitting a stacked regression ensemble from `stacks` package vignette and using `ensModelVis` for visualising the models.

Packages we will need:

```{r libs}
library(tidymodels)
library(stacks)
```


Dataset: predict `latency` based on other attributes in `tree_frogs` data exported with `stacks`. 

```{r}
data("tree_frogs",  package = "stacks")

# subset the data
tree_frogs <- tree_frogs %>%
  filter(!is.na(latency)) %>%
  select(-c(clutch, hatched))
```



Split the training data, generate resamples, set the recipe and metric.

```{r}

set.seed(1)
tree_frogs_split <- initial_split(tree_frogs)
tree_frogs_train <- training(tree_frogs_split)
tree_frogs_test  <- testing(tree_frogs_split)

set.seed(1)
folds <- vfold_cv(tree_frogs_train, v = 5)

tree_frogs_rec <- 
  recipe(latency ~ ., 
         data = tree_frogs_train)

metric <- metric_set(rmse)

ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
```

Fit a linear model and a support vector machine model (with hyperparameters to tune).


```{r}
# LINEAR REG
lin_reg_spec <-
  linear_reg() %>%
  set_engine("lm")

# extend the recipe
lin_reg_rec <-
  tree_frogs_rec %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors(), skip = TRUE)

# add both to a workflow
lin_reg_wflow <- 
  workflow() %>%
  add_model(lin_reg_spec) %>%
  add_recipe(lin_reg_rec)

# fit to the 5-fold cv
set.seed(2020)
lin_reg_res <- 
  fit_resamples(
    lin_reg_wflow,
    resamples = folds,
    metrics = metric,
    control = ctrl_res
  )

# SVM
svm_spec <- 
  svm_rbf(
    cost = tune("cost"), 
    rbf_sigma = tune("sigma")
  ) %>%
  set_engine("kernlab") %>%
  set_mode("regression")

# extend the recipe
svm_rec <-
  tree_frogs_rec %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors(), skip = TRUE) %>%
  step_impute_mean(all_numeric(), skip = TRUE) %>%
  step_corr(all_predictors(), skip = TRUE) %>%
  step_normalize(all_numeric(), skip = TRUE)

# add both to a workflow
svm_wflow <- 
  workflow() %>% 
  add_model(svm_spec) %>%
  add_recipe(svm_rec)

# tune cost and sigma and fit to the 5-fold cv
set.seed(2020)
svm_res <- 
  tune_grid(
    svm_wflow, 
    resamples = folds, 
    grid = 6,
    metrics = metric,
    control = ctrl_grid
  )

```

Use stacks to get the ensemble:

```{r}
tree_frogs_model_st <- 
  stacks() %>%
  add_candidates(lin_reg_res) %>%
  add_candidates(svm_res) %>%
  blend_predictions() %>%
  fit_members()
```


Predict with test data:



```{r}
member_preds <- 
  tree_frogs_test %>%
  select(latency) %>%
  bind_cols(predict(tree_frogs_model_st, tree_frogs_test, members = TRUE))
```

Evaluate RMSE from each model:

```{r}
map_dfr(member_preds, rmse, truth = latency, data = member_preds) %>%
  mutate(member = colnames(member_preds))
```

SVM does not make useful predictions here. We can see this from the RMSE and more clearly from the plots:

```{r}
plot_ensemble(truth = member_preds$latency, tibble_pred = member_preds %>% select(-latency))

plot_ensemble(truth = member_preds$latency, tibble_pred = member_preds %>% select(-latency), facet = TRUE)
```

